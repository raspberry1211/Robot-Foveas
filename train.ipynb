{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a013c1-a191-4b8f-b5c9-0b465e3fdb0e",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d64e41-edee-4d37-a548-12a126574efb",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63316d7c-62a7-4ca1-8765-56e2b3e16d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, Subset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "from FovConvNeXt.models import make_model\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af1ed0-d756-477c-b47b-2455fa56c818",
   "metadata": {},
   "source": [
    "## Step 2: Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c4be2-e31e-4461-b702-d2642ba81b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Print messages to let the user know how it's progressing\n",
    "    print('Beginning training...')\n",
    "    \n",
    "    # Training parameters\n",
    "    n_classes = 100 # Imagenet-100 has 100 classes\n",
    "    batch_size = 64\n",
    "    num_epochs = 200\n",
    "    learning_rate = 0.004\n",
    "    num_workers = 4\n",
    "    n_fixations = 1  # Number of fixations for the active vision model\n",
    "    max_grad_norm = 1.0  # Gradient clipping threshold\n",
    "    weight_decay = 0.005\n",
    "    \n",
    "    # Model parameters\n",
    "    radius = 0.4\n",
    "    block_sigma = 0.8\n",
    "    block_max_ord = 4\n",
    "    patch_sigma = 1.0\n",
    "    patch_max_ord = 4\n",
    "    ds_sigma = 0.6\n",
    "    ds_max_ord = 0\n",
    "\n",
    "    print('Loading data')\n",
    "    \n",
    "    # Dataset paths\n",
    "    DATA_PATH = \"/rhome/drfj2024/Robot-Foveas/data/imagenet-100/train.X\" # Path to all the training images. This later gets split into train & test sets\n",
    "    VAL_PATH = \"/rhome/drfj2024/Robot-Foveas/data/imagenet-100/val.X\"\n",
    "\n",
    "    # Training transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.TrivialAugmentWide(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Validation & Test transforms\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Load the full dataset\n",
    "    full_dataset = datasets.ImageFolder(DATA_PATH)\n",
    "\n",
    "    def split_dataset(dataset, train_size):\n",
    "        \"\"\"Split the dataset into training and test\"\"\"\n",
    "        total_size = len(dataset)\n",
    "        indices = np.arange(total_size)\n",
    "        np.random.shuffle(indices)\n",
    "        train_idx = int(total_size * train_size)\n",
    "        \n",
    "        train_indices = indices[:train_idx]\n",
    "        test_indices = indices[train_idx:]\n",
    "        \n",
    "        return train_indices, test_indices\n",
    "\n",
    "    # Custom dataset class to apply different transforms\n",
    "    class TransformDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, dataset, transform):\n",
    "            self.dataset = dataset\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __getitem__(self, index):\n",
    "            x, y = self.dataset[index]\n",
    "            return self.transform(x), y\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "    # Split the training dataset\n",
    "    train_indices, test_indices = split_dataset(\n",
    "        combined_train_dataset,\n",
    "        train_size=0.8, # 80% of dataset is used for training, 20% for testing\n",
    "    )\n",
    "\n",
    "    # Create subsets with appropriate transforms\n",
    "    train_dataset = TransformDataset(\n",
    "        Subset(combined_train_dataset, train_indices),\n",
    "        train_transform\n",
    "    )\n",
    "\n",
    "    test_dataset = TransformDataset(\n",
    "        Subset(combined_train_dataset, test_indices),\n",
    "        val_test_transform\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    print('Data loaded')\n",
    "\n",
    "    \n",
    "    # Create model with correct number of classes\n",
    "    model = make_model(\n",
    "        n_fixations=n_fixations,\n",
    "        n_classes=n_classes, \n",
    "        radius=radius,\n",
    "        block_sigma=block_sigma,\n",
    "        block_max_ord=block_max_ord,\n",
    "        patch_sigma=patch_sigma,\n",
    "        patch_max_ord=patch_max_ord,\n",
    "        ds_sigma=ds_sigma,\n",
    "        ds_max_ord=ds_max_ord\n",
    "    )\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer with increased weight decay\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1) \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    patience = 20\n",
    "    best_test_acc = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(\"Warning: NaN loss detected, skipping batch\")\n",
    "                continue\n",
    "                \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # for inputs, targets in test_loader:\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_total += targets.size(0)\n",
    "                test_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        test_acc = 100. * test_correct / test_total\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Acc: {test_acc:.2f}%')\n",
    "        \n",
    "        # Early stopping\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss/len(train_loader),\n",
    "                'test_loss': test_loss/len(test_loader),\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc\n",
    "            }, 'fixed_settings_best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss/len(train_loader),\n",
    "                'test_loss': test_loss/len(test_loader),\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc\n",
    "            }, f'fixed_settings_checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "    # Once training is complete, validate\n",
    "    # Load validation dataset\n",
    "    val_dataset = datasets.ImageFolder(VAL_PATH)\n",
    "\n",
    "    # Apply transform to validation dataset\n",
    "    val_dataset = TransformDataset(val_dataset, val_test_transform)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "                \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    print(f'Final Validation Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93456764-8079-41dd-ab48-b36f4e093c4a",
   "metadata": {},
   "source": [
    "## Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40cfe9-ac04-409c-96ca-a74c69701090",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EA_ready_kernel",
   "language": "python",
   "name": "ea_ready_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
